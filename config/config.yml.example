# Web Crawler Configuration File
# This is the main configuration file for the Python Web Crawler

# ============================================================================
# WEBSITE CONFIGURATION
# ============================================================================
website:
  # The main domain of the website (used for URL validation)
  base_domain: "your-site.com"
  
  # The base URL for the website
  base_url: "https://your-site.com"
  
  # Starting URL for the crawl (where the crawler begins)
  start_url: "https://your-site.com/docs"
  
  # URL patterns that are considered valid for crawling
  # The crawler will only follow links that match these patterns
  valid_url_patterns:
    - "/docs/"
    - "/wiki/"
    - "/spaces/"
  
  # URL patterns to exclude from crawling
  # Links matching these patterns will be ignored
  exclude_patterns:
    - "/admin"
    - "/login"
    - "/logout"
    - "action="
    - "?delete"
    - "/api/"

# ============================================================================
# CONFLUENCE API MODE (NEW!)
# ============================================================================
# The crawler automatically detects Confluence sites and switches to API mode
# when credentials are available in config/.env
#
# To enable Confluence API mode:
# 1. Create config/.env file (use config/.env.template as reference)
# 2. Add your Confluence credentials:
#    CONFLUENCE_BASE_URL=https://your-instance.atlassian.net
#    CONFLUENCE_EMAIL=your-email@example.com
#    CONFLUENCE_TOKEN=your_api_token_here
#
# Benefits of API mode:
# - Extracts 33+ metadata fields (versions, authors, history, labels)
# - Downloads all attachments automatically
# - Generates YAML metadata files
# - More accurate content extraction
# - Better performance with pagination
#
# Fallback: If no credentials are found, the crawler uses HTML mode
#
# See docs/CONFLUENCE_API_MODE.md for complete setup guide

# ============================================================================
# CRAWLING PARAMETERS
# ============================================================================
crawling:
  # Maximum depth to crawl (0 = only start URL, 1 = start + direct links, etc.)
  max_depth: 2
  
  # Space or section identifier (used for organizing output)
  space_name: "DOCS"
  
  # Number of parallel workers for downloading
  # Higher values = faster downloads but more server load
  max_workers: 8
  
  # Delay between requests in seconds (be respectful to the server)
  request_delay: 0.5
  
  # Timeout for individual requests in seconds
  request_timeout: 30
  
  # Maximum retries for failed requests
  max_retries: 3

# ============================================================================
# OUTPUT CONFIGURATION
# ============================================================================
output:
  # Output format: "html" or "markdown"
  format: "markdown"
  
  # Directory where downloaded content will be saved
  output_dir: "downloaded_content"
  
  # Directory for shared resources (CSS, images, etc.)
  resources_dir: "shared_resources"
  
  # Whether to create a separate directory for each space/section
  separate_spaces: true
  
  # Whether to preserve the original directory structure
  preserve_structure: true

# ============================================================================
# DATABASE CONFIGURATION
# ============================================================================
database:
  # SQLite database file path for storing crawl progress
  db_path: "crawler_data.db"
  
  # Whether to automatically migrate from JSON progress files
  auto_migrate_json: true
  
  # JSON backup settings (for migration)
  json_backup_dir: "json_backups"
  
  # Keep JSON backup after migration
  keep_json_backup: true
  
  # Database performance settings
  enable_wal_mode: true  # Write-Ahead Logging for better concurrency
  cache_size: 10000      # SQLite cache size in pages

# ============================================================================
# FILES CONFIGURATION
# ============================================================================
files:
  # Path to the cookies file (for authentication)
  cookies_file: "config/cookies.txt"
  
  # Whether to resume previous downloads (now handled by database)
  resume_downloads: true

# ============================================================================
# CONTENT PROCESSING
# ============================================================================
content:
  # Whether to remove JavaScript from HTML
  remove_javascript: true
  
  # Whether to clean HTML (remove unnecessary elements)
  clean_html: true
  
  # Elements to remove from HTML (for cleaning)
  remove_elements:
    - "script"
    - "noscript"
    - "iframe[src*='ads']"
    - ".advertisement"
    - "#sidebar"
  
  # Whether to download and save resources (CSS, images, etc.)
  download_resources: true
  
  # Types of resources to download
  resource_types:
    - "css"
    - "js"
    - "png"
    - "jpg"
    - "jpeg"
    - "gif"
    - "svg"
    - "ico"
    - "woff"
    - "woff2"
    - "ttf"
  
  # ---- Confluence API Mode Settings ----
  # These settings apply when using Confluence API crawler
  
  # Whether to download page attachments (Confluence API mode only)
  download_attachments: true
  
  # Whether to rewrite attachment URLs to local paths in HTML
  rewrite_attachment_urls: true
  
  # Whether to save raw JSON API responses (for debugging)
  save_json: false
  
  # Whether to save YAML metadata files (recommended)
  save_yaml: true
  
  # Whether to follow child pages (nested pages in Confluence)
  follow_child_pages: true
  
  # Maximum attachment size in MB (0 = no limit)
  max_attachment_size: 100

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================
logging:
  # Whether to enable verbose logging
  verbose: false
  
  # Whether to log resource downloads
  log_resources: false
  
  # Whether to log URL processing
  log_urls: true
  
  # Log file path (optional, if you want to save logs to file)
  # log_file: "crawler.log"

# ============================================================================
# ADVANCED CONFIGURATION
# ============================================================================
advanced:
  # User agent string for requests
  user_agent: "Python-WebCrawler/3.0"
  
  # Additional headers to send with requests
  headers:
    Accept: "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
    Accept-Language: "en-US,en;q=0.5"
    Accept-Encoding: "gzip, deflate"
    Connection: "keep-alive"
  
  # Whether to verify SSL certificates
  verify_ssl: true
  
  # Whether to follow redirects
  allow_redirects: true
  
  # Maximum size for individual files (in bytes, 0 = no limit)
  max_file_size: 0
  
  # CDN domains to consider as resources
  cdn_domains:
    - "cdn.jsdelivr.net"
    - "cdnjs.cloudflare.com"
    - "ajax.googleapis.com"
    - "fonts.googleapis.com"
    - "fonts.gstatic.com"

# ============================================================================
# EXAMPLES OF COMMON CONFIGURATIONS
# ============================================================================

# Example 1: Confluence Site
# website:
#   base_domain: "company.atlassian.net"
#   base_url: "https://company.atlassian.net"
#   start_url: "https://company.atlassian.net/wiki/spaces/DOCS/overview"
#   valid_url_patterns:
#     - "/wiki/spaces/DOCS/"
#   exclude_patterns:
#     - "action="
#     - "/admin"

# Example 2: Documentation Site
# website:
#   base_domain: "docs.example.com"
#   base_url: "https://docs.example.com"
#   start_url: "https://docs.example.com/v1/"
#   valid_url_patterns:
#     - "/v1/"
#   exclude_patterns:
#     - "/api/"

# Example 3: GitBook or Similar
# website:
#   base_domain: "company.gitbook.io"
#   base_url: "https://company.gitbook.io"
#   start_url: "https://company.gitbook.io/docs"
#   valid_url_patterns:
#     - "/docs"
#   exclude_patterns:
#     - "/edit"
#     - "/comments"
